{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytu46DP3cd_B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "6bTxAOSAcmD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train_cdc.csv\")   # replace path\n",
        "\n",
        "TARGET = \"price\"\n",
        "\n",
        "df[\"log_price\"] = np.log(df[TARGET])\n",
        "df = df.drop(columns=[TARGET, 'id', 'date', 'zipcode'])\n"
      ],
      "metadata": {
        "id": "SM_AQxiNcmAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "num_cols.remove(\"log_price\")\n",
        "\n",
        "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "print(\"Numerical columns:\", num_cols)\n",
        "print(\"Categorical columns:\", cat_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5hkieNkcl9S",
        "outputId": "136e0544-d3f7-4c28-bd29-c05d8273b044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical columns: ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
            "Categorical columns: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=[\"log_price\"])\n",
        "y = df[\"log_price\"].values\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "EuFdZExzcl6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_cols),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "8QBUrVKlcl3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_proc = preprocessor.fit_transform(X_train)\n",
        "X_val_proc   = preprocessor.transform(X_val)\n",
        "\n",
        "input_dim = X_train_proc.shape[1]"
      ],
      "metadata": {
        "id": "LwO_nvi9cl08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import issparse\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        if issparse(X):\n",
        "            X = X.toarray()\n",
        "\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "8Lbk3IvhclyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TabularDataset(X_train_proc, y_train)\n",
        "val_ds   = TabularDataset(X_val_proc, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "zzStxRZEcyks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TabularFCNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)"
      ],
      "metadata": {
        "id": "1dY3VYAUcyhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TabularFCNet(input_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "fa4pb7pOcyeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 200\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(Xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    model.eval()\n",
        "    val_preds, val_true = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            preds = model(Xb)\n",
        "\n",
        "            val_preds.append(preds.cpu().numpy())\n",
        "            val_true.append(yb.cpu().numpy())\n",
        "\n",
        "    val_preds = np.concatenate(val_preds)\n",
        "    val_true  = np.concatenate(val_true)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(val_true, val_preds))\n",
        "    r2   = r2_score(val_true, val_preds)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n",
        "        f\"Train MSE: {np.mean(train_losses):.4f} | \"\n",
        "        f\"Val RMSE (log): {rmse:.4f} | \"\n",
        "        f\"Val R²: {r2:.4f}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp3M0lWScybd",
        "outputId": "d66fa2d0-eff7-4e32-c48a-ee61f9f6993a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200] | Train MSE: 35.5511 | Val RMSE (log): 0.7284 | Val R²: -0.9228\n",
            "Epoch [2/200] | Train MSE: 1.4627 | Val RMSE (log): 0.6469 | Val R²: -0.5163\n",
            "Epoch [3/200] | Train MSE: 1.3132 | Val RMSE (log): 0.6242 | Val R²: -0.4119\n",
            "Epoch [4/200] | Train MSE: 1.1663 | Val RMSE (log): 0.6051 | Val R²: -0.3266\n",
            "Epoch [5/200] | Train MSE: 1.1501 | Val RMSE (log): 0.5385 | Val R²: -0.0509\n",
            "Epoch [6/200] | Train MSE: 1.0993 | Val RMSE (log): 0.4644 | Val R²: 0.2185\n",
            "Epoch [7/200] | Train MSE: 1.0448 | Val RMSE (log): 0.5081 | Val R²: 0.0644\n",
            "Epoch [8/200] | Train MSE: 0.9765 | Val RMSE (log): 0.4685 | Val R²: 0.2047\n",
            "Epoch [9/200] | Train MSE: 0.9533 | Val RMSE (log): 0.5399 | Val R²: -0.0561\n",
            "Epoch [10/200] | Train MSE: 0.9499 | Val RMSE (log): 0.5255 | Val R²: -0.0006\n",
            "Epoch [11/200] | Train MSE: 0.9227 | Val RMSE (log): 0.5189 | Val R²: 0.0244\n",
            "Epoch [12/200] | Train MSE: 0.8941 | Val RMSE (log): 0.3877 | Val R²: 0.4552\n",
            "Epoch [13/200] | Train MSE: 0.8711 | Val RMSE (log): 0.4424 | Val R²: 0.2907\n",
            "Epoch [14/200] | Train MSE: 0.8410 | Val RMSE (log): 0.3823 | Val R²: 0.4705\n",
            "Epoch [15/200] | Train MSE: 0.8044 | Val RMSE (log): 0.4149 | Val R²: 0.3762\n",
            "Epoch [16/200] | Train MSE: 0.7884 | Val RMSE (log): 0.4685 | Val R²: 0.2047\n",
            "Epoch [17/200] | Train MSE: 0.7738 | Val RMSE (log): 0.3730 | Val R²: 0.4957\n",
            "Epoch [18/200] | Train MSE: 0.7571 | Val RMSE (log): 0.4169 | Val R²: 0.3701\n",
            "Epoch [19/200] | Train MSE: 0.7696 | Val RMSE (log): 0.4525 | Val R²: 0.2580\n",
            "Epoch [20/200] | Train MSE: 0.7395 | Val RMSE (log): 0.3596 | Val R²: 0.5313\n",
            "Epoch [21/200] | Train MSE: 0.7196 | Val RMSE (log): 0.3568 | Val R²: 0.5387\n",
            "Epoch [22/200] | Train MSE: 0.7071 | Val RMSE (log): 0.3933 | Val R²: 0.4396\n",
            "Epoch [23/200] | Train MSE: 0.6762 | Val RMSE (log): 0.3666 | Val R²: 0.5130\n",
            "Epoch [24/200] | Train MSE: 0.6749 | Val RMSE (log): 0.3741 | Val R²: 0.4929\n",
            "Epoch [25/200] | Train MSE: 0.6748 | Val RMSE (log): 0.3697 | Val R²: 0.5048\n",
            "Epoch [26/200] | Train MSE: 0.6635 | Val RMSE (log): 0.3749 | Val R²: 0.4907\n",
            "Epoch [27/200] | Train MSE: 0.6503 | Val RMSE (log): 0.3770 | Val R²: 0.4850\n",
            "Epoch [28/200] | Train MSE: 0.6242 | Val RMSE (log): 0.3197 | Val R²: 0.6296\n",
            "Epoch [29/200] | Train MSE: 0.6292 | Val RMSE (log): 0.3864 | Val R²: 0.4589\n",
            "Epoch [30/200] | Train MSE: 0.6186 | Val RMSE (log): 0.4182 | Val R²: 0.3663\n",
            "Epoch [31/200] | Train MSE: 0.6159 | Val RMSE (log): 0.3328 | Val R²: 0.5987\n",
            "Epoch [32/200] | Train MSE: 0.5832 | Val RMSE (log): 0.4535 | Val R²: 0.2548\n",
            "Epoch [33/200] | Train MSE: 0.5808 | Val RMSE (log): 0.3789 | Val R²: 0.4798\n",
            "Epoch [34/200] | Train MSE: 0.5625 | Val RMSE (log): 0.3970 | Val R²: 0.4289\n",
            "Epoch [35/200] | Train MSE: 0.5622 | Val RMSE (log): 0.3148 | Val R²: 0.6410\n",
            "Epoch [36/200] | Train MSE: 0.5451 | Val RMSE (log): 0.2738 | Val R²: 0.7284\n",
            "Epoch [37/200] | Train MSE: 0.5395 | Val RMSE (log): 0.2888 | Val R²: 0.6979\n",
            "Epoch [38/200] | Train MSE: 0.5367 | Val RMSE (log): 0.2611 | Val R²: 0.7529\n",
            "Epoch [39/200] | Train MSE: 0.5212 | Val RMSE (log): 0.2825 | Val R²: 0.7108\n",
            "Epoch [40/200] | Train MSE: 0.5011 | Val RMSE (log): 0.2857 | Val R²: 0.7041\n",
            "Epoch [41/200] | Train MSE: 0.5100 | Val RMSE (log): 0.2862 | Val R²: 0.7031\n",
            "Epoch [42/200] | Train MSE: 0.4967 | Val RMSE (log): 0.3509 | Val R²: 0.5538\n",
            "Epoch [43/200] | Train MSE: 0.4719 | Val RMSE (log): 0.3334 | Val R²: 0.5972\n",
            "Epoch [44/200] | Train MSE: 0.4756 | Val RMSE (log): 0.2563 | Val R²: 0.7619\n",
            "Epoch [45/200] | Train MSE: 0.4772 | Val RMSE (log): 0.3467 | Val R²: 0.5644\n",
            "Epoch [46/200] | Train MSE: 0.4652 | Val RMSE (log): 0.3142 | Val R²: 0.6423\n",
            "Epoch [47/200] | Train MSE: 0.4479 | Val RMSE (log): 0.2467 | Val R²: 0.7794\n",
            "Epoch [48/200] | Train MSE: 0.4434 | Val RMSE (log): 0.2639 | Val R²: 0.7476\n",
            "Epoch [49/200] | Train MSE: 0.4402 | Val RMSE (log): 0.2338 | Val R²: 0.8019\n",
            "Epoch [50/200] | Train MSE: 0.4330 | Val RMSE (log): 0.2798 | Val R²: 0.7163\n",
            "Epoch [51/200] | Train MSE: 0.4167 | Val RMSE (log): 0.2382 | Val R²: 0.7943\n",
            "Epoch [52/200] | Train MSE: 0.4080 | Val RMSE (log): 0.2567 | Val R²: 0.7612\n",
            "Epoch [53/200] | Train MSE: 0.4037 | Val RMSE (log): 0.2355 | Val R²: 0.7991\n",
            "Epoch [54/200] | Train MSE: 0.4083 | Val RMSE (log): 0.2791 | Val R²: 0.7177\n",
            "Epoch [55/200] | Train MSE: 0.4122 | Val RMSE (log): 0.2424 | Val R²: 0.7871\n",
            "Epoch [56/200] | Train MSE: 0.3950 | Val RMSE (log): 0.2614 | Val R²: 0.7523\n",
            "Epoch [57/200] | Train MSE: 0.3764 | Val RMSE (log): 0.2421 | Val R²: 0.7876\n",
            "Epoch [58/200] | Train MSE: 0.3853 | Val RMSE (log): 0.2731 | Val R²: 0.7297\n",
            "Epoch [59/200] | Train MSE: 0.3811 | Val RMSE (log): 0.2230 | Val R²: 0.8199\n",
            "Epoch [60/200] | Train MSE: 0.3682 | Val RMSE (log): 0.2321 | Val R²: 0.8047\n",
            "Epoch [61/200] | Train MSE: 0.3591 | Val RMSE (log): 0.2927 | Val R²: 0.6895\n",
            "Epoch [62/200] | Train MSE: 0.3606 | Val RMSE (log): 0.2270 | Val R²: 0.8133\n",
            "Epoch [63/200] | Train MSE: 0.3508 | Val RMSE (log): 0.2386 | Val R²: 0.7938\n",
            "Epoch [64/200] | Train MSE: 0.3524 | Val RMSE (log): 0.2209 | Val R²: 0.8233\n",
            "Epoch [65/200] | Train MSE: 0.3392 | Val RMSE (log): 0.2702 | Val R²: 0.7355\n",
            "Epoch [66/200] | Train MSE: 0.3253 | Val RMSE (log): 0.2373 | Val R²: 0.7960\n",
            "Epoch [67/200] | Train MSE: 0.3347 | Val RMSE (log): 0.2871 | Val R²: 0.7014\n",
            "Epoch [68/200] | Train MSE: 0.3263 | Val RMSE (log): 0.2193 | Val R²: 0.8258\n",
            "Epoch [69/200] | Train MSE: 0.3188 | Val RMSE (log): 0.2298 | Val R²: 0.8087\n",
            "Epoch [70/200] | Train MSE: 0.3144 | Val RMSE (log): 0.2141 | Val R²: 0.8339\n",
            "Epoch [71/200] | Train MSE: 0.3102 | Val RMSE (log): 0.2112 | Val R²: 0.8384\n",
            "Epoch [72/200] | Train MSE: 0.3138 | Val RMSE (log): 0.2104 | Val R²: 0.8396\n",
            "Epoch [73/200] | Train MSE: 0.3067 | Val RMSE (log): 0.2526 | Val R²: 0.7688\n",
            "Epoch [74/200] | Train MSE: 0.3026 | Val RMSE (log): 0.2219 | Val R²: 0.8216\n",
            "Epoch [75/200] | Train MSE: 0.2959 | Val RMSE (log): 0.2140 | Val R²: 0.8341\n",
            "Epoch [76/200] | Train MSE: 0.3003 | Val RMSE (log): 0.2442 | Val R²: 0.7839\n",
            "Epoch [77/200] | Train MSE: 0.2879 | Val RMSE (log): 0.2075 | Val R²: 0.8440\n",
            "Epoch [78/200] | Train MSE: 0.2776 | Val RMSE (log): 0.2100 | Val R²: 0.8402\n",
            "Epoch [79/200] | Train MSE: 0.2816 | Val RMSE (log): 0.2215 | Val R²: 0.8222\n",
            "Epoch [80/200] | Train MSE: 0.2828 | Val RMSE (log): 0.3294 | Val R²: 0.6068\n",
            "Epoch [81/200] | Train MSE: 0.2759 | Val RMSE (log): 0.2197 | Val R²: 0.8251\n",
            "Epoch [82/200] | Train MSE: 0.2688 | Val RMSE (log): 0.2115 | Val R²: 0.8379\n",
            "Epoch [83/200] | Train MSE: 0.2609 | Val RMSE (log): 0.2241 | Val R²: 0.8180\n",
            "Epoch [84/200] | Train MSE: 0.2643 | Val RMSE (log): 0.2102 | Val R²: 0.8398\n",
            "Epoch [85/200] | Train MSE: 0.2670 | Val RMSE (log): 0.2100 | Val R²: 0.8402\n",
            "Epoch [86/200] | Train MSE: 0.2567 | Val RMSE (log): 0.2035 | Val R²: 0.8500\n",
            "Epoch [87/200] | Train MSE: 0.2552 | Val RMSE (log): 0.2079 | Val R²: 0.8434\n",
            "Epoch [88/200] | Train MSE: 0.2542 | Val RMSE (log): 0.2016 | Val R²: 0.8527\n",
            "Epoch [89/200] | Train MSE: 0.2498 | Val RMSE (log): 0.1986 | Val R²: 0.8570\n",
            "Epoch [90/200] | Train MSE: 0.2524 | Val RMSE (log): 0.2202 | Val R²: 0.8242\n",
            "Epoch [91/200] | Train MSE: 0.2447 | Val RMSE (log): 0.2080 | Val R²: 0.8433\n",
            "Epoch [92/200] | Train MSE: 0.2448 | Val RMSE (log): 0.2462 | Val R²: 0.7803\n",
            "Epoch [93/200] | Train MSE: 0.2397 | Val RMSE (log): 0.2192 | Val R²: 0.8259\n",
            "Epoch [94/200] | Train MSE: 0.2374 | Val RMSE (log): 0.2089 | Val R²: 0.8419\n",
            "Epoch [95/200] | Train MSE: 0.2333 | Val RMSE (log): 0.2136 | Val R²: 0.8346\n",
            "Epoch [96/200] | Train MSE: 0.2373 | Val RMSE (log): 0.1984 | Val R²: 0.8574\n",
            "Epoch [97/200] | Train MSE: 0.2279 | Val RMSE (log): 0.2630 | Val R²: 0.7494\n",
            "Epoch [98/200] | Train MSE: 0.2186 | Val RMSE (log): 0.1939 | Val R²: 0.8637\n",
            "Epoch [99/200] | Train MSE: 0.2298 | Val RMSE (log): 0.2060 | Val R²: 0.8462\n",
            "Epoch [100/200] | Train MSE: 0.2294 | Val RMSE (log): 0.2028 | Val R²: 0.8510\n",
            "Epoch [101/200] | Train MSE: 0.2274 | Val RMSE (log): 0.1933 | Val R²: 0.8646\n",
            "Epoch [102/200] | Train MSE: 0.2180 | Val RMSE (log): 0.2130 | Val R²: 0.8356\n",
            "Epoch [103/200] | Train MSE: 0.2182 | Val RMSE (log): 0.2322 | Val R²: 0.8047\n",
            "Epoch [104/200] | Train MSE: 0.2127 | Val RMSE (log): 0.2147 | Val R²: 0.8330\n",
            "Epoch [105/200] | Train MSE: 0.2182 | Val RMSE (log): 0.3000 | Val R²: 0.6739\n",
            "Epoch [106/200] | Train MSE: 0.2213 | Val RMSE (log): 0.2032 | Val R²: 0.8503\n",
            "Epoch [107/200] | Train MSE: 0.2062 | Val RMSE (log): 0.2011 | Val R²: 0.8534\n",
            "Epoch [108/200] | Train MSE: 0.2112 | Val RMSE (log): 0.1944 | Val R²: 0.8631\n",
            "Epoch [109/200] | Train MSE: 0.2058 | Val RMSE (log): 0.1977 | Val R²: 0.8584\n",
            "Epoch [110/200] | Train MSE: 0.2043 | Val RMSE (log): 0.1918 | Val R²: 0.8667\n",
            "Epoch [111/200] | Train MSE: 0.2088 | Val RMSE (log): 0.1891 | Val R²: 0.8705\n",
            "Epoch [112/200] | Train MSE: 0.2016 | Val RMSE (log): 0.1905 | Val R²: 0.8685\n",
            "Epoch [113/200] | Train MSE: 0.1997 | Val RMSE (log): 0.2171 | Val R²: 0.8293\n",
            "Epoch [114/200] | Train MSE: 0.2050 | Val RMSE (log): 0.1932 | Val R²: 0.8647\n",
            "Epoch [115/200] | Train MSE: 0.2019 | Val RMSE (log): 0.2167 | Val R²: 0.8298\n",
            "Epoch [116/200] | Train MSE: 0.1936 | Val RMSE (log): 0.1926 | Val R²: 0.8655\n",
            "Epoch [117/200] | Train MSE: 0.1989 | Val RMSE (log): 0.1936 | Val R²: 0.8641\n",
            "Epoch [118/200] | Train MSE: 0.1940 | Val RMSE (log): 0.1978 | Val R²: 0.8583\n",
            "Epoch [119/200] | Train MSE: 0.1954 | Val RMSE (log): 0.2221 | Val R²: 0.8213\n",
            "Epoch [120/200] | Train MSE: 0.1925 | Val RMSE (log): 0.1938 | Val R²: 0.8639\n",
            "Epoch [121/200] | Train MSE: 0.1931 | Val RMSE (log): 0.2296 | Val R²: 0.8090\n",
            "Epoch [122/200] | Train MSE: 0.1913 | Val RMSE (log): 0.2019 | Val R²: 0.8522\n",
            "Epoch [123/200] | Train MSE: 0.1861 | Val RMSE (log): 0.1932 | Val R²: 0.8648\n",
            "Epoch [124/200] | Train MSE: 0.1869 | Val RMSE (log): 0.1940 | Val R²: 0.8636\n",
            "Epoch [125/200] | Train MSE: 0.1824 | Val RMSE (log): 0.1857 | Val R²: 0.8751\n",
            "Epoch [126/200] | Train MSE: 0.1830 | Val RMSE (log): 0.2203 | Val R²: 0.8242\n",
            "Epoch [127/200] | Train MSE: 0.1845 | Val RMSE (log): 0.2077 | Val R²: 0.8437\n",
            "Epoch [128/200] | Train MSE: 0.1822 | Val RMSE (log): 0.2027 | Val R²: 0.8510\n",
            "Epoch [129/200] | Train MSE: 0.1820 | Val RMSE (log): 0.1868 | Val R²: 0.8735\n",
            "Epoch [130/200] | Train MSE: 0.1745 | Val RMSE (log): 0.2354 | Val R²: 0.7993\n",
            "Epoch [131/200] | Train MSE: 0.1789 | Val RMSE (log): 0.1871 | Val R²: 0.8732\n",
            "Epoch [132/200] | Train MSE: 0.1768 | Val RMSE (log): 0.2073 | Val R²: 0.8442\n",
            "Epoch [133/200] | Train MSE: 0.1714 | Val RMSE (log): 0.2376 | Val R²: 0.7954\n",
            "Epoch [134/200] | Train MSE: 0.1792 | Val RMSE (log): 0.1877 | Val R²: 0.8723\n",
            "Epoch [135/200] | Train MSE: 0.1788 | Val RMSE (log): 0.1844 | Val R²: 0.8767\n",
            "Epoch [136/200] | Train MSE: 0.1770 | Val RMSE (log): 0.1945 | Val R²: 0.8629\n",
            "Epoch [137/200] | Train MSE: 0.1734 | Val RMSE (log): 0.1876 | Val R²: 0.8724\n",
            "Epoch [138/200] | Train MSE: 0.1686 | Val RMSE (log): 0.1880 | Val R²: 0.8719\n",
            "Epoch [139/200] | Train MSE: 0.1700 | Val RMSE (log): 0.1886 | Val R²: 0.8711\n",
            "Epoch [140/200] | Train MSE: 0.1686 | Val RMSE (log): 0.2076 | Val R²: 0.8439\n",
            "Epoch [141/200] | Train MSE: 0.1650 | Val RMSE (log): 0.1856 | Val R²: 0.8752\n",
            "Epoch [142/200] | Train MSE: 0.1667 | Val RMSE (log): 0.1945 | Val R²: 0.8629\n",
            "Epoch [143/200] | Train MSE: 0.1650 | Val RMSE (log): 0.1863 | Val R²: 0.8742\n",
            "Epoch [144/200] | Train MSE: 0.1675 | Val RMSE (log): 0.1929 | Val R²: 0.8652\n",
            "Epoch [145/200] | Train MSE: 0.1654 | Val RMSE (log): 0.1887 | Val R²: 0.8709\n",
            "Epoch [146/200] | Train MSE: 0.1628 | Val RMSE (log): 0.1952 | Val R²: 0.8620\n",
            "Epoch [147/200] | Train MSE: 0.1626 | Val RMSE (log): 0.1897 | Val R²: 0.8696\n",
            "Epoch [148/200] | Train MSE: 0.1585 | Val RMSE (log): 0.1913 | Val R²: 0.8673\n",
            "Epoch [149/200] | Train MSE: 0.1624 | Val RMSE (log): 0.1884 | Val R²: 0.8714\n",
            "Epoch [150/200] | Train MSE: 0.1629 | Val RMSE (log): 0.1858 | Val R²: 0.8749\n",
            "Epoch [151/200] | Train MSE: 0.1619 | Val RMSE (log): 0.1877 | Val R²: 0.8724\n",
            "Epoch [152/200] | Train MSE: 0.1586 | Val RMSE (log): 0.2032 | Val R²: 0.8504\n",
            "Epoch [153/200] | Train MSE: 0.1602 | Val RMSE (log): 0.1945 | Val R²: 0.8629\n",
            "Epoch [154/200] | Train MSE: 0.1601 | Val RMSE (log): 0.1987 | Val R²: 0.8570\n",
            "Epoch [155/200] | Train MSE: 0.1580 | Val RMSE (log): 0.1863 | Val R²: 0.8742\n",
            "Epoch [156/200] | Train MSE: 0.1543 | Val RMSE (log): 0.1885 | Val R²: 0.8712\n",
            "Epoch [157/200] | Train MSE: 0.1545 | Val RMSE (log): 0.2034 | Val R²: 0.8501\n",
            "Epoch [158/200] | Train MSE: 0.1485 | Val RMSE (log): 0.2197 | Val R²: 0.8251\n",
            "Epoch [159/200] | Train MSE: 0.1521 | Val RMSE (log): 0.1815 | Val R²: 0.8806\n",
            "Epoch [160/200] | Train MSE: 0.1483 | Val RMSE (log): 0.1905 | Val R²: 0.8685\n",
            "Epoch [161/200] | Train MSE: 0.1495 | Val RMSE (log): 0.1830 | Val R²: 0.8787\n",
            "Epoch [162/200] | Train MSE: 0.1534 | Val RMSE (log): 0.1860 | Val R²: 0.8746\n",
            "Epoch [163/200] | Train MSE: 0.1472 | Val RMSE (log): 0.1850 | Val R²: 0.8760\n",
            "Epoch [164/200] | Train MSE: 0.1502 | Val RMSE (log): 0.1901 | Val R²: 0.8690\n",
            "Epoch [165/200] | Train MSE: 0.1462 | Val RMSE (log): 0.1825 | Val R²: 0.8793\n",
            "Epoch [166/200] | Train MSE: 0.1467 | Val RMSE (log): 0.1972 | Val R²: 0.8591\n",
            "Epoch [167/200] | Train MSE: 0.1447 | Val RMSE (log): 0.2380 | Val R²: 0.7947\n",
            "Epoch [168/200] | Train MSE: 0.1430 | Val RMSE (log): 0.1830 | Val R²: 0.8787\n",
            "Epoch [169/200] | Train MSE: 0.1463 | Val RMSE (log): 0.1938 | Val R²: 0.8638\n",
            "Epoch [170/200] | Train MSE: 0.1403 | Val RMSE (log): 0.2620 | Val R²: 0.7512\n",
            "Epoch [171/200] | Train MSE: 0.1491 | Val RMSE (log): 0.1858 | Val R²: 0.8749\n",
            "Epoch [172/200] | Train MSE: 0.1429 | Val RMSE (log): 0.1913 | Val R²: 0.8674\n",
            "Epoch [173/200] | Train MSE: 0.1369 | Val RMSE (log): 0.2095 | Val R²: 0.8410\n",
            "Epoch [174/200] | Train MSE: 0.1413 | Val RMSE (log): 0.2086 | Val R²: 0.8424\n",
            "Epoch [175/200] | Train MSE: 0.1371 | Val RMSE (log): 0.1873 | Val R²: 0.8729\n",
            "Epoch [176/200] | Train MSE: 0.1368 | Val RMSE (log): 0.2181 | Val R²: 0.8276\n",
            "Epoch [177/200] | Train MSE: 0.1411 | Val RMSE (log): 0.1838 | Val R²: 0.8776\n",
            "Epoch [178/200] | Train MSE: 0.1425 | Val RMSE (log): 0.1998 | Val R²: 0.8554\n",
            "Epoch [179/200] | Train MSE: 0.1352 | Val RMSE (log): 0.2035 | Val R²: 0.8500\n",
            "Epoch [180/200] | Train MSE: 0.1362 | Val RMSE (log): 0.1847 | Val R²: 0.8763\n",
            "Epoch [181/200] | Train MSE: 0.1370 | Val RMSE (log): 0.1930 | Val R²: 0.8650\n",
            "Epoch [182/200] | Train MSE: 0.1351 | Val RMSE (log): 0.1924 | Val R²: 0.8658\n",
            "Epoch [183/200] | Train MSE: 0.1327 | Val RMSE (log): 0.1891 | Val R²: 0.8704\n",
            "Epoch [184/200] | Train MSE: 0.1356 | Val RMSE (log): 0.1836 | Val R²: 0.8779\n",
            "Epoch [185/200] | Train MSE: 0.1330 | Val RMSE (log): 0.2008 | Val R²: 0.8539\n",
            "Epoch [186/200] | Train MSE: 0.1308 | Val RMSE (log): 0.3169 | Val R²: 0.6360\n",
            "Epoch [187/200] | Train MSE: 0.1319 | Val RMSE (log): 0.1819 | Val R²: 0.8800\n",
            "Epoch [188/200] | Train MSE: 0.1337 | Val RMSE (log): 0.1832 | Val R²: 0.8784\n",
            "Epoch [189/200] | Train MSE: 0.1357 | Val RMSE (log): 0.2316 | Val R²: 0.8057\n",
            "Epoch [190/200] | Train MSE: 0.1305 | Val RMSE (log): 0.1837 | Val R²: 0.8777\n",
            "Epoch [191/200] | Train MSE: 0.1268 | Val RMSE (log): 0.1853 | Val R²: 0.8756\n",
            "Epoch [192/200] | Train MSE: 0.1264 | Val RMSE (log): 0.1889 | Val R²: 0.8708\n",
            "Epoch [193/200] | Train MSE: 0.1276 | Val RMSE (log): 0.1807 | Val R²: 0.8816\n",
            "Epoch [194/200] | Train MSE: 0.1292 | Val RMSE (log): 0.1946 | Val R²: 0.8628\n",
            "Epoch [195/200] | Train MSE: 0.1226 | Val RMSE (log): 0.2090 | Val R²: 0.8418\n",
            "Epoch [196/200] | Train MSE: 0.1247 | Val RMSE (log): 0.1941 | Val R²: 0.8635\n",
            "Epoch [197/200] | Train MSE: 0.1254 | Val RMSE (log): 0.1920 | Val R²: 0.8665\n",
            "Epoch [198/200] | Train MSE: 0.1234 | Val RMSE (log): 0.1809 | Val R²: 0.8814\n",
            "Epoch [199/200] | Train MSE: 0.1238 | Val RMSE (log): 0.1875 | Val R²: 0.8726\n",
            "Epoch [200/200] | Train MSE: 0.1234 | Val RMSE (log): 0.1900 | Val R²: 0.8692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_price_true = np.exp(val_true)\n",
        "val_price_pred = np.exp(val_preds)\n",
        "\n",
        "rmse_price = np.sqrt(mean_squared_error(val_price_true, val_price_pred))\n",
        "r2_price   = r2_score(val_price_true, val_price_pred)\n",
        "\n",
        "print(\"Final RMSE (price):\", rmse_price)\n",
        "print(\"Final R² (price):\", r2_price)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CF2EZ_Ec9LK",
        "outputId": "c7739e66-d048-431d-efa5-203d480aa62a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final RMSE (price): 129204.18370935207\n",
            "Final R² (price): 0.8669701814651489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P6td1DN3fCV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}